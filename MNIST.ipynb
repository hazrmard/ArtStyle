{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "% reload_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "from lib.pipeline import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "This notebook uses the `MNIST` dataset to illustrate the structure of code for this project in the context of the `pytorch` echosystem.\n",
    "\n",
    "## The pytorch sequence\n",
    "\n",
    "* **Dataset**: A `Dataset` is an object that supports indexing and length operations. `Dataset[i]` will return a tuple of the ith training instance and its label. The returned instance and label will be in a pytorch-friently format i.e. Tensors. `Dataset[i] = (x:Tensor, y: Tensor)`.\n",
    "\n",
    "* **Dataloader**: A `Dataloader` is a wrapper around a `Dataset`. It uses multiple processes to quickly read the `Dataset` at multiple indices. It can also convert single instances into mini-batches, shuffle data etc. A `Dataloader` can be iterated over to provide training and label data ready for comsumption by the network. So `for x, y in Dataloader` will yield a Tensor containing a minibatch of training examples and their corresponding labels.\n",
    "\n",
    "* **Module**: A `Module` is a neural network (or any differentiable function) that can accept a minibatch and output predictions.\n",
    "\n",
    "* **Criterion**: A class that describes the loss function the `Module` will use the judge how bad its outputs are compared to the true labels. That is, `loss = criterion(prediction, labels)` will return the loss function computed for that minibatch.\n",
    "\n",
    "* **Optimizer**: A class that describes how to update `Module` weights given the loss. It takes as arguments a reference to the `Module` parameters, and other hyperparameters like learning rate and momentum.\n",
    "\n",
    "The standard training loop goes like:\n",
    "\n",
    "```\n",
    "Create a dataset that can return individual examples,\n",
    "Give that dataset to a dataloader to generate batches,\n",
    "Create network = Module(),\n",
    "Create loss, optimization functions from hyperparameters,\n",
    "Transfer network to GPU if required\n",
    "\n",
    "For epoch in number of epochs:\n",
    "    For x, y in dataloader:\n",
    "        transfer x, y to GPU if required\n",
    "        zero-out network gradients from previous iteration (optimizer.zero_grad()),\n",
    "        generate outputs (predictions = network(x))\n",
    "        compute loss = criterion(predictions, y)\n",
    "        backpropagate loss (loss.backward())\n",
    "        optimize parameters (optimizer.step())\n",
    "```\n",
    "\n",
    "## Sequence simplified\n",
    "\n",
    "This project simplifies the sequence by encapsulating `torch` functions. The entirety of a model (from data input to training) is described by a single module in `lib/pipeline`. Each module file contains:\n",
    "\n",
    "* **Net()**: A `Module` that describes the network architecture.\n",
    "* **Data()**: A `Dataset` that reads and indexes individual examples in torch-friendly format.\n",
    "* **Model()**: A class that is instantiated with `net`, `optimizer`, `criterion`. It has a:\n",
    "  * `train()` method that includes the training loop logic.\n",
    "  * `precict()` method that returns `net` output for a minibatch.\n",
    "  * `evaluate()` method that calculates top-N accuracy over a dataset.\n",
    "  * `save()` and `load()` methods that backup and restore network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a torch.utils.data.DataSet instance. Can be indexed to lazily\n",
    "# load instance/label. Ideally all datasets should be subclassed from\n",
    "# DataSet.\n",
    "traindata = mnist.Data(root='~/Downloads/MNIST', train=True)\n",
    "\n",
    "print('Instances: {}'.format(len(traindata)))\n",
    "\n",
    "fig, (row1, row2) = plt.subplots(nrows=2, ncols=5)\n",
    "for i, ax in enumerate((*row1, *row2)):\n",
    "    x, y = traindata[i]  # x.shape == (1, 28, 28), y.shape == (1,)\n",
    "    ax.imshow(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "LRATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "USE_CUDA = True\n",
    "\n",
    "# Instantiate the defined network and hyperparameters\n",
    "net = mnist.Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LRATE, momentum=MOMENTUM)\n",
    "\n",
    "# Create a model that combines it all.\n",
    "model = mnist.Model(net=net, criterion=criterion, optimizer=optimizer, cuda=USE_CUDA)\n",
    "\n",
    "# Train model with verbal updates\n",
    "model.train(dataset=traindata, batch_size=BATCH_SIZE, epochs=EPOCHS, verbosity='auto')\n",
    "\n",
    "# Save model weights after training\n",
    "model.save('mnist.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore model for evaluation (just an example - evaluation can be done without save/load)\n",
    "net = mnist.Net()\n",
    "criterion = None    # not needed for evaluation\n",
    "optimizer = None    # not needed for evaluation\n",
    "\n",
    "# Create a model that combines it all.\n",
    "model_restored = mnist.Model(net=net, criterion=criterion, optimizer=optimizer, cuda=USE_CUDA)\n",
    "\n",
    "# load model\n",
    "model_restored.load('mnist.pt')\n",
    "\n",
    "# Evaluate learned model on test dataset\n",
    "testdata = mnist.Data(root='~/Downloads/MNIST', train=False)\n",
    "model_restored.evaluate(testdata, batch_size=100, topn=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
